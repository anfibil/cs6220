{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is the supervised learning task of predicting the value of a categorical outcome (\"class\") variable, _y_, given real and/or categorical input (\"feature\") data, _X_. The objective of classification is to learn a model of the data that can be use to predict the correct class variable for new or unseen feature data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of classification algorithms exist. These algorithms have been developed under varying assumptions and employ different concepts. Each algorithm may interact with data differently based upon the size, dimensionality, and noise of the dataset, among other characteristics. These algorithms may have varying degrees of interpretability, variability, and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use the `scikit-learn` (sklearn) package to expore the use of several classification algorithms. Let's fetch the Iris dataset from the UCI Machine Learning repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "col_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Name']\n",
    "df_iris = pd.read_csv(file_url, names=col_names, header=None)\n",
    "\n",
    "X = df_iris.iloc[:, :2]  # features\n",
    "y = df_iris.iloc[:, -1]  # class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print some of the feature and the class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.head())\n",
    "print()\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `sklearn` classification algorithms require that the class values be represented as numbers. We can use the `LabelEncoder` function to convert the string class labels to corresponding integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply some classification methods to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _k_-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest classification algorithms is the _k_-nearest neighbors algorithm. Let's import it from `sklearn`. We can call the method with several available inputs. Here, we specify the number of neighbors _k_ to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf_knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every classifier in `sklearn` has a `fit()` method. For a supervised learning algorithm, which learns to map features _X_ to classes _y_, we must input the corresponding _X_ and _y_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call an algorithm that has been fit (or parameterized) to input data a \"model.\" Can now use the fitted model to predict the class _y_ given new features _X_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll simply see how well the classifier can predict the data on which it was fit. We can predict on feature data using the fitted model's `predict()` method, the output of which are corresponding class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_knn.predict(X)\n",
    "clf_knn.kneighbors(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the actual values, `y`, to the predicted values, `y_pred`. Here we compute the fraction of times that they match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(y == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try visualizing the results. We can use another Python package, `matplotlib`, to generate these visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the color scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color maps.\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a meshgrid of matrix of feature values. For each feature, we'll create a list of evenly spaced values. With two features, this will produce a grid of points. We can calculate the predicted class values for each point in this grid to create a plot with colored decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mesh of shape [x_min, m_max]x[y_min, y_max].\n",
    "h = .05  # step size in the mesh\n",
    "x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
    "y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the class predictions and create the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a class value for each point in the mesh.\n",
    "Z = clf_knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Use the predictions to create a color plot.\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points.\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Nearest Neighbors Classifier\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try fitting an visualizing another classifier, the Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_per = Perceptron(max_iter=1000)\n",
    "clf_per.fit(X, y)\n",
    "\n",
    "# Predict a class value for each point in the mesh.\n",
    "Z = clf_per.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Use the predictions to create a color plot.\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points.\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Perceptron Classifier\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when building a model, we're typically interested in performance on data we haven't yet seen. A simple model that simply memorizes the training data may perform well on the data we've seen but poorly on unseen data. Unless we can evaluate a model on unseen data, we don't know how accurate its predictions may be.\n",
    "\n",
    "To address this issue, we can separate the data into training and testing sets. We can fit the model on the training data and then evaluate it on the separate testing data. `sklearn` provides a convenient function to do this. Let's save 20% of our data for testing, and use the rest for fitting our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "clf_per.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = clf_knn.predict(X_test)\n",
    "y_pred_per = clf_per.predict(X_test)\n",
    "\n",
    "print(np.average(y_test == y_pred_knn))\n",
    "print(np.average(y_test == y_pred_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try viewing the results from a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "clf_lr.fit(X, y)\n",
    "\n",
    "# Predict a class value for each point in the mesh.\n",
    "Z = clf_lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Use the predictions to create a color plot.\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot the training points.\n",
    "plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold)\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"Logistic Regression Classifier\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try viewing the online results of the perceptron classifier. As an iterative classifier, we can use `matplotlib`'s animation functions to view how the algorithm learns over the course of many iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "# Set up the figure, the axis, and plot elements.\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_title(\"Perceptron Classifier\")\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initialization function.\"\"\"\n",
    "    clf_per.partial_fit(X, y)\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"Animation function. This is called sequentially.\"\"\"\n",
    "    clf_per.partial_fit(X, y)\n",
    "\n",
    "    Z = clf_per.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot the training points.\n",
    "    ax.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold)\n",
    "\n",
    "    return ax\n",
    "\n",
    "# Call the animator.\n",
    "anim = animation.FuncAnimation(fig, func=animate, frames=25, init_func=init)\n",
    "\n",
    "HTML(anim.to_html5_video());  # requires ffmpeg to be installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice applying what we've learned to a house price dataset for Ames, Iowa. We'll try to predict whether the house price is \"high\" or \"low\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop or fill missing values. Consider how to handle the following columns, which have many missing values: `Alley`, `Fence`, `Fireplace Qu`, `Misc Feature`, `Pool QC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Alley', 'Fence', 'Fireplace Qu', 'Misc Feature', 'Pool QC'], 1)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separate the features and the class. Here, our class is the last column `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transform the features to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bin the SalePrice into two outcomes, 'high' or 'low'. Make them roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Low', 'High']\n",
    "y_bin = pd.qcut(df['SalePrice'], 2, labels=class_names)\n",
    "print(X.shape, y_bin.shape)\n",
    "print(y_bin.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split the dataset into 80% training data and 20% testing data. Each row should consist of the features, `X`, and the binned sales price, `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fit three models: a 1-nearest neighbors classifier, a 10-nearest neighbors classifier, and a 100-nearest neighbors classifier. Fit them all on the training data. Use all of the feature data to predict the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=10)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=100)\n",
    "\n",
    "clf1.fit(X_train, y_train);\n",
    "clf2.fit(X_train, y_train);\n",
    "clf3.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compare the accuracy of the _k_-nearest neighbors classifiers on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = clf1.predict(X_test)\n",
    "y_pred2 = clf2.predict(X_test)\n",
    "y_pred3 = clf3.predict(X_test)\n",
    "\n",
    "print(\"KNN (k=1):  \", round(np.average(y_test == y_pred1), 3))\n",
    "print(\"KNN (k=10): \", round(np.average(y_test == y_pred2), 3))\n",
    "print(\"KNN (k=100):\", round(np.average(y_test == y_pred3), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Now fit a perceptron classifier and logistic regression classifier on the training data. Compare the accuray of the perceptron, logistic regression, and the KNN classifiers. Which performs best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = Perceptron(max_iter=1000)\n",
    "clf5 = LogisticRegression()\n",
    "\n",
    "clf4.fit(X_train, y_train)\n",
    "clf5.fit(X_train, y_train)\n",
    "\n",
    "y_pred4 = clf4.predict(X_test)\n",
    "y_pred5 = clf5.predict(X_test)\n",
    "print(\"Perceptron:\", round(np.average(y_test == y_pred4), 3))\n",
    "print(\"Logistic Regression:\", round(np.average(y_test == y_pred5), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
